{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iv6bjjqyGmqV"
      },
      "source": [
        "# Homework 6 - Generative Adversarial Network\n",
        "This is the sample code for hw6 of 2022 Machine Learning course in National Taiwan University.\n",
        "\n",
        "In this sample code, there are 5 sections:\n",
        "1. Environment setting\n",
        "2. Dataset preparation\n",
        "3. Model setting\n",
        "4. Train\n",
        "5. Inference\n",
        "\n",
        "Your goal is to do anime face generation, if you have any question, please discuss at NTU COOL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<font size = 10 color = red> 以后记住，所有在forward中自己生成的大矩阵要直接在GPU上生成, i.e. `torch.tensor([···], devise = 'cuda')`</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnp-5lUFLak7"
      },
      "source": [
        "# Environment setting\n",
        "In this section, we will prepare for the dataset and set some environment variable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qhoMUt9LniJ"
      },
      "source": [
        "## Download Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "AaJRTJEFLrND"
      },
      "outputs": [],
      "source": [
        "data_path = '../faces'\n",
        "# get dataset from huggingface hub\n",
        "#!curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | bash\n",
        "#!apt-get install git-lfs\n",
        "#!git lfs install\n",
        "#!git clone https://huggingface.co/datasets/LeoFeng/MLHW_6\n",
        "#!unzip ./MLHW_6/faces.zip -d ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBkkAB9sO3R4"
      },
      "source": [
        "## Other setting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Qxf1TXTLO6Ek"
      },
      "outputs": [],
      "source": [
        "# import module\n",
        "import os\n",
        "import glob\n",
        "import random\n",
        "from datetime import datetime\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch import optim\n",
        "from torch.autograd import Variable, grad # 这个grad函数是pytorch的自动求导函数，用于计算gradient penalty\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import logging\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "# seed setting\n",
        "def same_seeds(seed):\n",
        "    # Python built-in random module\n",
        "    random.seed(seed)\n",
        "    # Numpy\n",
        "    np.random.seed(seed)\n",
        "    # Torch\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "same_seeds(4595)\n",
        "workspace_dir = '.'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eg2qsevzOeQT"
      },
      "source": [
        "# Dataset preparation\n",
        "In this section, we prepare for the dataset for Pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UT6s1x92OudB"
      },
      "source": [
        "## Create dataset for Pytorch\n",
        "\n",
        "In order to unified image information, we use the transform function to:\n",
        "1. Resize image to 64x64\n",
        "2. Normalize the image\n",
        "\n",
        "This CrypkoDataset class will be use in Section 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9MsHqaglOywi"
      },
      "outputs": [],
      "source": [
        "# prepare for CrypkoDataset\n",
        "\n",
        "class CrypkoDataset(Dataset):\n",
        "    def __init__(self, fnames, transform):\n",
        "        self.transform = transform\n",
        "        self.fnames = fnames\n",
        "        self.num_samples = len(self.fnames)\n",
        "\n",
        "    def __getitem__(self,idx):\n",
        "        fname = self.fnames[idx]\n",
        "        img = torchvision.io.read_image(fname)\n",
        "        img = self.transform(img)\n",
        "        return img\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "\n",
        "def get_dataset(root):\n",
        "    fnames = glob.glob(os.path.join(root, '*'))\n",
        "    compose = [\n",
        "        transforms.ToPILImage(),\n",
        "        transforms.Resize((64, 64)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
        "    ]\n",
        "    transform = transforms.Compose(compose)\n",
        "    dataset = CrypkoDataset(fnames, transform)\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPMZTwAiQSnx"
      },
      "source": [
        "## Show the image\n",
        "Show some sample in the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "rX5-Q71TOyy4",
        "outputId": "5ccce544-982c-4635-c88a-3a0290985669"
      },
      "outputs": [],
      "source": [
        "temp_dataset = get_dataset(os.path.join(workspace_dir, data_path))\n",
        "\n",
        "images = [temp_dataset[i] for i in range(4)]\n",
        "grid_img = torchvision.utils.make_grid(images, nrow=4)\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.imshow(grid_img.permute(1, 2, 0))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgV-jpcfQwEM"
      },
      "source": [
        "# Model setting\n",
        "In this section, we will create models and trainer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EY4rAlw8RNhG"
      },
      "source": [
        "## Create model\n",
        "In this section, we will create models for Generator and Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mapping（StyleGAN）\n",
        "\n",
        "class MappingGenerator(nn.Module):\n",
        "    \"\"\"\n",
        "    Input: 无\n",
        "    Output shape: (batch, style_dim)\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        '''\n",
        "        暂时random input与GAN的输入共用\n",
        "        '''\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.z_dim = self.config[\"z_dim\"]\n",
        "        self.style_dim = self.config[\"style_dim\"]\n",
        "        self.model = nn.Sequential (\n",
        "            self.FC_block(self.z_dim, 256),\n",
        "            self.FC_block(256, 512),\n",
        "            self.FC_block(512, 512),\n",
        "            self.FC_block(512, 512),\n",
        "            self.FC_block(512, 512),\n",
        "            self.FC_block(512, 512),\n",
        "            self.FC_block(512, 512),\n",
        "            nn.Linear(512, self.style_dim) # 输出\n",
        "        )\n",
        "\n",
        "    def FC_block(self, input_dim, output_dim):\n",
        "        return nn.Sequential(\n",
        "            nn.Linear(input_dim, output_dim),\n",
        "            nn.LayerNorm(output_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, random_input):\n",
        "        return self.model.forward(random_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# AdIN 归一化层\n",
        "class AdIN(nn.Module):\n",
        "    def __init__(self, mapping_dim, channel) -> None:\n",
        "        '''\n",
        "        用于计算AdIN的公式\n",
        "        '''\n",
        "        super().__init__()\n",
        "        # 为每个通道生成缩放因子和偏移因子\n",
        "        self.affine_scale = nn.Linear(mapping_dim, channel)\n",
        "        self.affine_shift = nn.Linear(mapping_dim, channel)\n",
        "    def forward(self, x : torch.Tensor, mapping : torch.Tensor):\n",
        "        '''\n",
        "        输入的x应该是 [B, C, N, N]\n",
        "        '''\n",
        "        B, C, _, _ = x.shape\n",
        "        A_scale = self.affine_scale(mapping).view(B, C, 1, 1)\n",
        "        A_shift = self.affine_shift(mapping).view(B, C, 1, 1)\n",
        "        var = x.var([2, 3], keepdim=True) + 1e-8  # 避免除以零\n",
        "        normalized = x / torch.sqrt(var)\n",
        "        out = A_scale * normalized + A_shift\n",
        "        return out\n",
        "        \n",
        "\n",
        "\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# NoiseLayer\n",
        "\n",
        "class NoiseLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Input : 无\n",
        "    Output : 与给定的维度相同的高斯噪声\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.a = nn.Parameter(torch.tensor(torch.randn(1, 1)), requires_grad = True) # 可学习权重\n",
        "\n",
        "    def forward(self, x : torch.Tensor):\n",
        "        noise = torch.normal(mean=0.0, std=1.0, size=x.shape, device = 'cuda') # 这样才快！！！！\n",
        "        return x + self.a * noise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8dfregFtRVGo"
      },
      "outputs": [],
      "source": [
        "# Generator\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    \"\"\"\n",
        "    Input shape: (batch, in_dim)\n",
        "    Output shape: (batch, 3, 64, 64)\n",
        "    \"\"\"\n",
        "    def __init__(self, in_dim, config, feature_dim=64):\n",
        "        super().__init__()\n",
        "        self.feature_dim = feature_dim\n",
        "        self.mapping_dim = config[\"style_dim\"]\n",
        "        #input: (batch, 100)\n",
        "        self.l1 = nn.Sequential(\n",
        "            nn.Linear(in_dim, feature_dim * 8 * 4 * 4, bias=False),\n",
        "            nn.BatchNorm1d(feature_dim * 8 * 4 * 4),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "       \n",
        "        self.l21 = self.dconv_bn_relu(feature_dim * 8, feature_dim * 4)             #(batch, feature_dim * 16, 8, 8)\n",
        "        self.l21_adin = AdIN(mapping_dim = self.mapping_dim, channel = feature_dim * 4) # outputsize: feature_dim * 4\n",
        "        self.l22 = self.dconv_bn_relu(feature_dim * 4, feature_dim * 2)             #(batch, feature_dim * 16, 16, 16)\n",
        "        self.l22_adin = AdIN(mapping_dim = self.mapping_dim, channel = feature_dim * 2)\n",
        "        self.l23 = self.dconv_bn_relu(feature_dim * 2, feature_dim)                 #(batch, feature_dim * 16, 32, 32)\n",
        "        self.l23_adin = AdIN(mapping_dim = self.mapping_dim, channel = feature_dim)\n",
        "        \n",
        "        self.l3 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(feature_dim, 3, kernel_size=5, stride=2,\n",
        "                               padding=2, output_padding=1, bias=False), # 注意，在生成的时候我们用转置卷积！！！！，用于上采样\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "        self.apply(weights_init)\n",
        "    def dconv_bn_relu(self, in_dim, out_dim):\n",
        "        return nn.Sequential(\n",
        "            NoiseLayer(), # 自己定义的NoiseLayer， 每一层都加一个噪声\n",
        "            nn.ConvTranspose2d(in_dim, out_dim, kernel_size=5, stride=2,\n",
        "                               padding=2, output_padding=1, bias=False),        #double height and width\n",
        "            #nn.BatchNorm2d(out_dim),\n",
        "            nn.ReLU(True), # inplace = true, 为了减少内存，反卷积非常非常耗内存！！！！\n",
        "        )\n",
        "    \n",
        "    def forward(self, x, mapping): # 为了Style\n",
        "        B = mapping.shape[0] # 获取mapping的Batch维度作为对输入的常数x的复制（对所有batch都要同一个输入！）\n",
        "        x = x.repeat(B, 1)\n",
        "        y = self.l1(x) # 先对输入进行一个Linear transform\n",
        "        y = y.view(y.size(0), -1, 4, 4)\n",
        "        y = self.l21(y)\n",
        "        y = self.l21_adin(y, mapping)\n",
        "        y = self.l22(y)\n",
        "        y = self.l22_adin(y, mapping)\n",
        "        y = self.l23(y)\n",
        "        y = self.l23_adin(y, mapping)\n",
        "        y = self.l3(y)\n",
        "        return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IHykBwExRr0_"
      },
      "outputs": [],
      "source": [
        "# Discriminator\n",
        "class Discriminator(nn.Module):\n",
        "    \"\"\"\n",
        "    Input shape: (batch, 3, 64, 64)\n",
        "    Output shape: (batch)\n",
        "    \"\"\"\n",
        "    def __init__(self, config, in_dim, feature_dim=64):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        #input: (batch, 3, 64, 64)\n",
        "        \"\"\"\n",
        "        NOTE FOR SETTING DISCRIMINATOR:\n",
        "\n",
        "        Remove last sigmoid layer for WGAN\n",
        "        \"\"\"\n",
        "        self.config = config # 记录具体是什么GAN\n",
        "        self.mapping_dim = config[\"style_dim\"]\n",
        "        self.l1 = nn.Sequential(\n",
        "            nn.Conv2d(in_dim, feature_dim, kernel_size=4, stride=2, padding=1), #(batch, 3, 32, 32)\n",
        "            nn.LeakyReLU(0.2),\n",
        "        )\n",
        "        self.l21 = self.conv_bn_lrelu(feature_dim, feature_dim * 2)                   #(batch, 3, 16, 16)\n",
        "        self.l21_adin = AdIN(self.mapping_dim, feature_dim * 2) \n",
        "        self.l22 = self.conv_bn_lrelu(feature_dim * 2, feature_dim * 4)               #(batch, 3, 8, 8)\n",
        "        self.l22_adin = AdIN(self.mapping_dim, feature_dim * 4) \n",
        "        self.l23 = self.conv_bn_lrelu(feature_dim * 4, feature_dim * 8)               #(batch, 3, 4, 4)\n",
        "        self.l23_adin = AdIN(self.mapping_dim, feature_dim * 8) \n",
        "        if (self.config[\"model_type\"] == 'GAN'):\n",
        "            self.l3 = nn.Sequential(\n",
        "                nn.Conv2d(feature_dim * 8, 1, kernel_size=4, stride=1, padding=0),\n",
        "                nn.Sigmoid()\n",
        "            )\n",
        "        else: # 所有WGAN都不能有Sigmoid\n",
        "            self.l3 = nn.Sequential(\n",
        "                nn.Conv2d(feature_dim * 8, 1, kernel_size=4, stride=1, padding=0),\n",
        "            )\n",
        "        self.apply(weights_init)\n",
        "    def conv_bn_lrelu(self, in_dim, out_dim):\n",
        "        \n",
        "        '''\n",
        "        对于WGAN-gp的判别器，是不能加 batchNorm 的，\n",
        "        原因很简单， 是因为WGAN-gp的惩罚项计算中，惩罚的是单个数据的gradient norm，如果使用 batchNorm，就会扰乱这种惩罚，让这种特别的惩罚失效。 \n",
        "        当然你可以绕过 batchNorm, 使用 layerNorm 或者 InstanceNorm\n",
        "        '''\n",
        "        \"\"\"\n",
        "        NOTE FOR SETTING DISCRIMINATOR:\n",
        "\n",
        "        You can't use nn.Batchnorm for WGAN-GP\n",
        "        Use nn.InstanceNorm2d instead\n",
        "        \"\"\"\n",
        "        if (self.config[\"model_type\"] == 'WGAN-GP'):\n",
        "            return nn.Sequential(\n",
        "                NoiseLayer(), # 自己定义的NoiseLayer， 每一层都加一个噪声\n",
        "                nn.Conv2d(in_dim, out_dim, 4, 2, 1),\n",
        "                #nn.InstanceNorm2d(out_dim), # StyleGAN前一定一定，不能用batchnorm！！！！！！！！！！！\n",
        "                nn.LeakyReLU(0.2),\n",
        "            )\n",
        "        \n",
        "        return nn.Sequential( # 如果不是\n",
        "            NoiseLayer(), # 自己定义的NoiseLayer， 每一层都加一个噪声\n",
        "            nn.Conv2d(in_dim, out_dim, 4, 2, 1),\n",
        "            #nn.BatchNorm2d(out_dim),\n",
        "            nn.LeakyReLU(0.2),\n",
        "        )\n",
        "    \n",
        "    def forward(self, x, mapping): # mapping 为了style\n",
        "        y = self.l1(x)\n",
        "        y = self.l21(y)\n",
        "        y = self.l21_adin(y, mapping)\n",
        "        y = self.l22(y)\n",
        "        y = self.l22_adin(y, mapping)\n",
        "        y = self.l23(y)\n",
        "        y = self.l23_adin(y, mapping)\n",
        "        y = self.l3(y)\n",
        "        y = y.view(-1)\n",
        "        return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hb7Y38bsR35o"
      },
      "outputs": [],
      "source": [
        "# setting for weight init function\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        m.weight.data.normal_(0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        m.weight.data.normal_(1.0, 0.02)\n",
        "        m.bias.data.fill_(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eC-6M2P3SAu9"
      },
      "source": [
        "## Create trainer\n",
        "In this section, we will create a trainer which contains following functions:\n",
        "1. prepare_environment: prepare the overall environment, construct the models, create directory for the log and ckpt\n",
        "2. train: train for generator and discriminator, you can try to modify the code here to construct WGAN or WGAN-GP\n",
        "3. inference: after training, you can pass the generator ckpt path into it and the function will save the result for you"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f8ajFDWBTRzn"
      },
      "outputs": [],
      "source": [
        "global_time = '' # 记录登记时间以在最后加载check point\n",
        "class TrainerGAN():\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.M_g = MappingGenerator(config = self.config)\n",
        "        self.G = Generator(100, config)\n",
        "        self.D = Discriminator(config = self.config, in_dim = 3)\n",
        "        self.static_z = torch.randn(1, self.config[\"z_dim\"], device = 'cuda') # for Style （注意所有batch共用一个）\n",
        "\n",
        "        self.loss = nn.BCELoss() # 用BCELoss！！！\n",
        "\n",
        "        \"\"\"\n",
        "        NOTE FOR SETTING OPTIMIZER:\n",
        "\n",
        "        GAN: use Adam optimizer\n",
        "        WGAN: use RMSprop optimizer\n",
        "        WGAN-GP: use Adam optimizer\n",
        "        \"\"\"\n",
        "        self.opt_M_g = torch.optim.Adam(self.M_g.parameters(), lr=self.config[\"lr\"], weight_decay = self.config[\"weight_decay\"], betas=(0.5, 0.999))\n",
        "        if(config[\"model_type\"] == 'GAN'):\n",
        "            self.opt_D = torch.optim.Adam(self.D.parameters(), lr=self.config[\"lr\"], weight_decay = self.config[\"weight_decay\"], betas=(0.5, 0.999))\n",
        "            self.opt_G = torch.optim.Adam(self.G.parameters(), lr=self.config[\"lr\"], weight_decay = self.config[\"weight_decay\"], betas=(0.5, 0.999))\n",
        "        if(config[\"model_type\"] == 'WGAN'):\n",
        "            self.opt_D = torch.optim.RMSprop(self.D.parameters(), lr=self.config[\"lr\"], weight_decay = self.config[\"weight_decay\"])\n",
        "            self.opt_G = torch.optim.RMSprop(self.G.parameters(), lr=self.config[\"lr\"], weight_decay = self.config[\"weight_decay\"])\n",
        "        if(config[\"model_type\"] == 'WGAN-GP'):\n",
        "            self.opt_D = torch.optim.Adam(self.D.parameters(), lr=self.config[\"lr\"], weight_decay = self.config[\"weight_decay\"], betas=(0.5, 0.999))\n",
        "            self.opt_G = torch.optim.Adam(self.G.parameters(), lr=self.config[\"lr\"], weight_decay = self.config[\"weight_decay\"], betas=(0.5, 0.999))\n",
        "\n",
        "        self.dataloader = None\n",
        "        self.log_dir = os.path.join(self.config[\"workspace_dir\"], 'logs')\n",
        "        self.ckpt_dir = os.path.join(self.config[\"workspace_dir\"], 'checkpoints')\n",
        "\n",
        "        FORMAT = '%(asctime)s - %(levelname)s: %(message)s'\n",
        "        logging.basicConfig(level=logging.INFO,\n",
        "                            format=FORMAT,\n",
        "                            datefmt='%Y-%m-%d %H:%M')\n",
        "\n",
        "        self.steps = 0\n",
        "        self.z_samples = torch.randn(100, self.config[\"z_dim\"]).cuda()\n",
        "\n",
        "    def prepare_environment(self):\n",
        "        \"\"\"\n",
        "        Use this funciton to prepare function\n",
        "        \"\"\"\n",
        "        global global_time\n",
        "        os.makedirs(self.log_dir, exist_ok=True)\n",
        "        os.makedirs(self.ckpt_dir, exist_ok=True)\n",
        "\n",
        "        # update dir by time\n",
        "        time = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
        "        global_time = time\n",
        "        self.log_dir = os.path.join(self.log_dir, time+f'_{self.config[\"model_type\"]}')\n",
        "        self.ckpt_dir = os.path.join(self.ckpt_dir, time+f'_{self.config[\"model_type\"]}')\n",
        "        os.makedirs(self.log_dir)\n",
        "        os.makedirs(self.ckpt_dir)\n",
        "\n",
        "        # create dataset by the above function\n",
        "        dataset = get_dataset(os.path.join(self.config[\"workspace_dir\"], data_path))\n",
        "        self.dataloader = DataLoader(dataset, batch_size=self.config[\"batch_size\"], shuffle=True, num_workers=2)\n",
        "\n",
        "        # model preparation\n",
        "        self.G = self.G.cuda()\n",
        "        self.D = self.D.cuda()\n",
        "        self.M_g = self.M_g.cuda()\n",
        "        self.G.train()\n",
        "        self.D.train()\n",
        "        self.M_g.train()\n",
        "    def gp(self, r_imgs : torch.Tensor, f_imgs : torch.Tensor, mapping): # 注意，这个Variable封装是老旧的实现，可以不用了现在\n",
        "        \"\"\"\n",
        "        Implement gradient penalty function\n",
        "        r_img : [batch, 3, 64, 64]\n",
        "        \"\"\"\n",
        "        lamb = self.config[\"gp_config\"][\"lambda\"]\n",
        "        eps = random.random() # 从 0 ～ 1 中随机取eps噪声\n",
        "\n",
        "        '''\n",
        "        为了gradient penalty 我们甚至还需要再前向传播一次判别器\n",
        "        '''\n",
        "\n",
        "        x_sharp : torch.Tensor = eps * r_imgs + (1 - eps) * f_imgs # 随机权重加权得出x\n",
        "        x_sharp.requires_grad_() # 需要求导\n",
        "        x_sharp.cuda()\n",
        "        D_x_sharp = self.D.forward(x_sharp, mapping) # 前向传播\n",
        "        grad_D_x_sharp = grad( # autograd.grad, 输出的是输出对于输入的tensor梯度\n",
        "            outputs = D_x_sharp, # outputs：这是你想要计算梯度的输出变量。在WGAN-GP的上下文中，outputs将是判别器对插值样本的输出。这个输出是需要对其进行微分的函数。\n",
        "            inputs = x_sharp, # inputs：这些是你需要计算其相对于outputs的梯度的输入变量。在WGAN-GP中，inputs将是插值样本本身。你感兴趣的是如何根据这些插值样本调整判别器的输出。\n",
        "            grad_outputs=torch.ones_like(D_x_sharp).cuda(), # 这是对outputs梯度的权重。\n",
        "                                                              # 在大多数情况下，如果outputs是标量（即单个数值），你可以简单地将其设置为torch.ones_like(outputs)，\n",
        "                                                              # 这意味着梯度不会被加权。在WGAN-GP的应用中，这通常是你所需要的，因为我们对计算梯度的绝对值感兴趣。\n",
        "            create_graph=True, # 当这个参数设置为True时，会构建用于计算梯度的计算图，这允许你随后对这些梯度再次进行微分（即进行二次微分）。在WGAN-GP中，这是必要的，因为梯度惩罚涉及到梯度的梯度（即梯度的范数的梯度）\n",
        "            retain_graph=True, # 如果设置为True，这个参数允许你多次使用grad函数，而不销毁中间计算图。在某些情况下，比如在计算梯度惩罚时，你可能需要保留计算图，以便进行额外的操作。\n",
        "            only_inputs=True # 当设置为True时，grad函数只会返回指定inputs的梯度，而不是所有可能的自动微分变量的梯度。在大多数情况下，这正是你所需要的，因为你通常只对特定输入的梯度感兴趣。\n",
        "        )[0] #取第一个就行了\n",
        "        gp = ((grad_D_x_sharp.norm(p=2, dim=(1, 2)) - 1)**2).mean().cuda() # 计算每梯度的L2范数, 求平均\n",
        "        return lamb * gp #正则化值\n",
        "\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"\n",
        "        Use this function to train generator and discriminator\n",
        "        \"\"\"\n",
        "        self.prepare_environment()\n",
        "\n",
        "        for e, epoch in enumerate(range(self.config[\"n_epoch\"])):\n",
        "            progress_bar = tqdm(self.dataloader)\n",
        "            progress_bar.set_description(f\"Epoch {e+1}\")\n",
        "            for i, data in enumerate(progress_bar):\n",
        "                imgs = data.cuda()\n",
        "                bs = imgs.size(0)\n",
        "\n",
        "                # *********************\n",
        "                # *    Train D        *\n",
        "                # *********************\n",
        "                # 此时mapping应该没有梯度\n",
        "                z = torch.randn(bs, self.config[\"z_dim\"], device = 'cuda') # 这里用 variable 是为了兼容旧版pytroch，以前pytorch的tensor不带自动求导功能的，要封装成一个variable才行！\n",
        "                self.M_g.eval() # 记得改成eval！！！\n",
        "                with torch.no_grad(): # mapping此时不计算梯度\n",
        "                    mapping = self.M_g(z)\n",
        "                self.M_g.train()\n",
        "                r_imgs = imgs.cuda() # r_imgs: 真实的img\n",
        "                f_imgs = self.G(self.static_z, mapping) # f_imgs: 生成的假img (styleGAN保留固定值)\n",
        "                r_label = torch.ones((bs)).cuda()\n",
        "                f_label = torch.zeros((bs)).cuda()\n",
        "\n",
        "\n",
        "                # Discriminator forwarding\n",
        "                r_logit = self.D(r_imgs, mapping)\n",
        "                f_logit = self.D(f_imgs, mapping)\n",
        "\n",
        "                \"\"\"\n",
        "                NOTE FOR SETTING DISCRIMINATOR LOSS:\n",
        "\n",
        "                GAN:\n",
        "                    loss_D = (r_loss + f_loss)/2\n",
        "                WGAN:\n",
        "                    loss_D = -torch.mean(r_logit) + torch.mean(f_logit)\n",
        "                WGAN-GP:\n",
        "                    gradient_penalty = self.gp(r_imgs, f_imgs)\n",
        "                    loss_D = -torch.mean(r_logit) + torch.mean(f_logit) + gradient_penalty\n",
        "                \"\"\"\n",
        "                # Loss for discriminator\n",
        "                if(self.config[\"model_type\"] == 'GAN'):\n",
        "                    r_loss = self.loss(r_logit, r_label)\n",
        "                    f_loss = self.loss(f_logit, f_label)\n",
        "                    loss_D = (r_loss + f_loss) / 2\n",
        "                elif(self.config[\"model_type\"] == 'WGAN'):\n",
        "                    loss_D = -torch.mean(r_logit) + torch.mean(f_logit)\n",
        "                elif(self.config[\"model_type\"] == 'WGAN-GP'):\n",
        "                    gradient_penalty = self.gp(r_imgs, f_imgs, mapping)\n",
        "                    loss_D =  -torch.mean(r_logit) + torch.mean(f_logit) + gradient_penalty\n",
        "                else:\n",
        "                    raise NameError(\"please cheke the config of 'model_type' \")\n",
        "\n",
        "                # Discriminator backwarding\n",
        "                self.D.zero_grad()\n",
        "                loss_D.backward()\n",
        "                self.opt_D.step()\n",
        "\n",
        "                \"\"\"\n",
        "                NOTE FOR SETTING WEIGHT CLIP:\n",
        "\n",
        "                WGAN: below code\n",
        "                \"\"\"\n",
        "                if (self.config[\"model_type\"] == 'WGAN'):\n",
        "                    for p in self.D.parameters():\n",
        "                        p.data.clamp_(-self.config[\"clip_value\"], self.config[\"clip_value\"])\n",
        "\n",
        "\n",
        "\n",
        "                # *********************\n",
        "                # *    Train G        *\n",
        "                # *********************\n",
        "                if self.steps % self.config[\"n_critic\"] == 0:\n",
        "                    # Generate some fake images.\n",
        "                    z = torch.randn(bs, self.config[\"z_dim\"]).cuda()\n",
        "                    f_imgs = self.G(self.static_z, mapping)\n",
        "\n",
        "                    # Generator forwarding\n",
        "                    f_logit = self.D(f_imgs, mapping)\n",
        "\n",
        "\n",
        "                    \"\"\"\n",
        "                    NOTE FOR SETTING LOSS FOR GENERATOR:\n",
        "\n",
        "                    GAN: loss_G = self.loss(f_logit, r_label)\n",
        "                    WGAN: loss_G = -torch.mean(self.D(f_imgs))\n",
        "                    WGAN-GP: loss_G = -torch.mean(self.D(f_imgs))\n",
        "                    \"\"\"\n",
        "                    # Loss for the generator.\n",
        "                    if (self.config[\"model_type\"] == 'GAN'):\n",
        "                        loss_G = self.loss(f_logit, r_label)\n",
        "                    elif(self.config[\"model_type\"] == 'WGAN'):\n",
        "                        loss_G = -torch.mean(self.D(f_imgs, mapping))\n",
        "                    elif(self.config[\"model_type\"] == 'WGAN-GP'):\n",
        "                        loss_G = -torch.mean(self.D(f_imgs, mapping))\n",
        "                    else:\n",
        "                        raise NameError(\"please cheke the config of 'model_type' \")\n",
        "\n",
        "                    # Generator backwarding\n",
        "                    self.G.zero_grad()\n",
        "                    self.M_g.zero_grad() # 注意，只用G的梯度！\n",
        "                    loss_G.backward()\n",
        "                    self.opt_G.step()\n",
        "                    self.opt_M_g.step()\n",
        "\n",
        "                if self.steps % 10 == 0:\n",
        "                    progress_bar.set_postfix(loss_G=loss_G.item(), loss_D=loss_D.item())\n",
        "                self.steps += 1\n",
        "\n",
        "            self.G.eval()\n",
        "            self.M_g.eval()\n",
        "            with torch.no_grad():\n",
        "                mapping = self.M_g(self.z_samples) # ramdom 输入要过一次mapping\n",
        "                f_imgs_sample = (self.G(self.static_z, mapping).data + 1) / 2.0 # 用一个常数输入z\n",
        "                filename = os.path.join(self.log_dir, f'Epoch_{epoch+1:03d}.jpg')\n",
        "                torchvision.utils.save_image(f_imgs_sample, filename, nrow=10)\n",
        "                logging.info(f'Save some samples to {filename}.')\n",
        "\n",
        "            # Show some images during training.\n",
        "            if ((e + 1) % 5 == 0 or e == 0 ): # 每隔上5次打印一次\n",
        "                grid_img = torchvision.utils.make_grid(f_imgs_sample.cpu(), nrow=10)\n",
        "                plt.figure(figsize=(10,10))\n",
        "                plt.imshow(grid_img.permute(1, 2, 0))\n",
        "                plt.show()\n",
        "\n",
        "            self.G.train()\n",
        "            self.M_g.train() # 记得改回train\n",
        "\n",
        "            if (e+1) % 10 == 0 or e == 0:\n",
        "                # Save the checkpoints.\n",
        "                torch.save(self.static_z,os.path.join(self.ckpt_dir, f'static_z_{e}.pth')) # 多存 \n",
        "                torch.save(self.M_g.state_dict(), os.path.join(self.ckpt_dir, f'M_g_{e}.pth'))\n",
        "                torch.save(self.G.state_dict(), os.path.join(self.ckpt_dir, f'G_{e}.pth'))\n",
        "                torch.save(self.D.state_dict(), os.path.join(self.ckpt_dir, f'D_{e}.pth'))\n",
        "\n",
        "        logging.info('Finish training')\n",
        "\n",
        "    def inference(self, G_path, static_z_path, M_g_path, n_generate=1000, n_output=30, show=False):\n",
        "        \"\"\"\n",
        "        1. G_path is the path for Generator ckpt\n",
        "        2. You can use this function to generate final answer\n",
        "        \"\"\"\n",
        "\n",
        "        self.G.load_state_dict(torch.load(G_path))\n",
        "        self.M_g.load_state_dict(torch.load(M_g_path))\n",
        "        self.static_z = torch.load(static_z_path)\n",
        "        self.G.cuda()\n",
        "        self.G.eval()\n",
        "        z = torch.randn(n_generate, self.config[\"z_dim\"]).cuda()\n",
        "        mapping = self.M_g.forward(random_input = z)\n",
        "        imgs = (self.G(self.static_z, mapping).data + 1) / 2.0\n",
        "\n",
        "        os.makedirs('output', exist_ok=True)\n",
        "        for i in range(n_generate):\n",
        "            torchvision.utils.save_image(imgs[i], f'output/{i+1}.jpg')\n",
        "\n",
        "        if show:\n",
        "            row, col = n_output//10 + 1, 10\n",
        "            grid_img = torchvision.utils.make_grid(imgs[:n_output].cpu(), nrow=row)\n",
        "            plt.figure(figsize=(row, col))\n",
        "            plt.imshow(grid_img.permute(1, 2, 0))\n",
        "            plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uf8BdVoYNJ8"
      },
      "source": [
        "# Train\n",
        "In this section, we will first set the config for trainer, then use it to train generator and discriminator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykjfugCdYmYS"
      },
      "source": [
        "## Set config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jg4YdRVPYJSj"
      },
      "outputs": [],
      "source": [
        "gp_config = {\n",
        "    \"lambda\" : 10,\n",
        "}\n",
        "\n",
        "config = {\n",
        "    \"is_style\" : True,\n",
        "    \"model_type\": \"WGAN-GP\",\n",
        "    \"clip_value\": 1, # only for WGAN, 设定clip值\n",
        "    \"batch_size\": 128,\n",
        "    \"lr\": 0.0001,\n",
        "    \"weight_decay\" : 1e-9,\n",
        "    \"n_epoch\": 200,\n",
        "    \"n_critic\": 1,\n",
        "    \"z_dim\": 100,\n",
        "    \"style_dim\" : 512, # 解耦后的style特征\n",
        "    \"gp_config\" : gp_config, # 用于gradient_pernalty的config\n",
        "    \"workspace_dir\": workspace_dir, # define in the environment setting\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntn56Ffvip-x"
      },
      "source": [
        "## Start to train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 648
        },
        "id": "NTHoXrLUYJUn",
        "outputId": "ef81adc1-56f4-4181-e0d1-c1490d7e9266"
      },
      "outputs": [],
      "source": [
        "trainer = TrainerGAN(config)\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4g3_RUzYix0W"
      },
      "source": [
        "# Inference\n",
        "In this section, we will use trainer to train model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6hdMgj_i3kk"
      },
      "source": [
        "## Inference through trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "72EEf52FrOCp"
      },
      "outputs": [],
      "source": [
        "# save the 1000 images into ./output folder\n",
        "which_epoch = 199 # 哪个epoch?\n",
        "G_path = f'{workspace_dir}/checkpoints/{global_time}_{config[\"model_type\"]}/G_{which_epoch}.pth'\n",
        "static_z_path = f'{workspace_dir}/checkpoints/{global_time}_{config[\"model_type\"]}/static_z_{which_epoch}.pth'\n",
        "M_g_path = f'{workspace_dir}/checkpoints/{global_time}_{config[\"model_type\"]}/M_g_{which_epoch}.pth'\n",
        "\n",
        "trainer.inference(G_path = G_path, static_z_path = static_z_path, M_g_path = M_g_path) # you have to modify the path when running this line"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WuoaEVUgk7oZ"
      },
      "source": [
        "## Prepare .tar file for submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QI2cnbbWlA3Z",
        "outputId": "7e2d3e13-250b-4820-b5bb-acaacb6398e4"
      },
      "outputs": [],
      "source": [
        "%cd output\n",
        "!tar -zcf ../submission.tgz *.jpg\n",
        "%cd .."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
